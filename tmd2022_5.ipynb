{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maskot1977/tmd2022/blob/jAYCHc8S/tmd2022_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI12Ihi9fzd3"
      },
      "source": [
        "「AI創薬・ケモインフォマティクス入門」講義資料　（講師：小寺正明）\n",
        "\n",
        "3月11日（土）19:40～21:10　第5回  「計算機実験1」\n",
        "\n",
        "# RDKit インストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accYXhCvKdua"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit-pypi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7H-VM39gAj0"
      },
      "source": [
        "# 化合物データ取得"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW7Vb-U_gDU7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# csvからのデータ読み込み_\n",
        "url = \"https://raw.githubusercontent.com/maskot1977/toydata/main/data/data_18.csv\"\n",
        "database1 = pd.read_csv(url)\n",
        "database1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ko2ZXTy09qO"
      },
      "source": [
        "# 回帰問題用目的変数とその分布"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyk48sWf08sq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title(\"Melting point as continus value Y1\")\n",
        "Y1 = database1[\"Melting point\"]\n",
        "Y1.hist(bins=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Ht5RHEgOef"
      },
      "source": [
        "# 分類用目的変数とその分布（練習のため）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcB7mfrStMMC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "Y2 = pd.DataFrame(\n",
        "    np.where(\n",
        "        database1[\"Melting point\"]\n",
        "        > database1[\"Melting point\"].describe().median() * 0.9,\n",
        "        1,\n",
        "        0,\n",
        "    ),\n",
        "    columns=[\"Melting point as discrete value Y2\"],\n",
        ")\n",
        "Y2.hist(bins=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFG1_oayKdui"
      },
      "source": [
        "# バイアスのある分類用目的変数とその分布（練習のため）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypmo6pJggN8o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "Y3 = pd.DataFrame(\n",
        "    np.where(\n",
        "        database1[\"Melting point\"]\n",
        "        > database1[\"Melting point\"].describe().median() * 2.2,\n",
        "        1,\n",
        "        0,\n",
        "    ),\n",
        "    columns=[\"Melting point as biased discrete value Y3\"],\n",
        ")\n",
        "Y3.hist(bins=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qreJv2rYJKg"
      },
      "source": [
        "# RDKit supporter\n",
        "\n",
        "RDKit supporter は、RDKit や ML 周りで便利な関数やクラスを私が書き溜めたものです。インストールしてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmpj76TGboXi"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/maskot1977/rdkit_supporter.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sz0nlqCkoxy"
      },
      "source": [
        "# RDKit 記述子"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eynPQyDecJ4j"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import rdkit\n",
        "from rdkit_supporter.descriptors import calc_descriptors\n",
        "\n",
        "rdkit_df = calc_descriptors(database1[\"Open Babel SMILES\"])\n",
        "display(rdkit_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soO9nM_z3ybI"
      },
      "source": [
        "# 説明変数\n",
        "\n",
        "今回は、説明変数 X として RDKit descriptors を用います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eddngk41m_JO"
      },
      "outputs": [],
      "source": [
        "X = rdkit_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLAxNRSbKdun"
      },
      "source": [
        "# 欠損値の補間の例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4fqDYrVBYVG"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer()\n",
        "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsrvdlOUIhTO"
      },
      "source": [
        "# 多様体学習 (Manifold Learning)\n",
        "\n",
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72oDXa1aF-Dh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "manifold = PCA()\n",
        "embedding = manifold.fit_transform(X)\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\")\n",
        "plt.xlabel(\"PC1({:.2f}%)\".format(manifold.explained_variance_ratio_[0] * 100))\n",
        "plt.ylabel(\"PC2({:.2f}%)\".format(manifold.explained_variance_ratio_[1] * 100))\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0nsKF2W7HEv"
      },
      "source": [
        "## 前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LunXUwAJlvG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "manifold = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA())])\n",
        "embedding = manifold.fit_transform(X)\n",
        "plt.scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\")\n",
        "plt.xlabel(\"PC1({:.2f}%)\".format(manifold[\"pca\"].explained_variance_ratio_[0] * 100))\n",
        "plt.ylabel(\"PC2({:.2f}%)\".format(manifold[\"pca\"].explained_variance_ratio_[1] * 100))\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9PPVvW9Y2v"
      },
      "source": [
        "## 目的変数で色付け"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxiSSIfqOflT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "manifold = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", PCA())])\n",
        "embedding = manifold.fit_transform(X)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "axes[0].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\")\n",
        "axes[1].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y2.values.flatten())\n",
        "axes[2].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y3.values.flatten())\n",
        "axes[0].set_xlabel(\n",
        "    \"PC1({:.2f}%)\".format(manifold[\"pca\"].explained_variance_ratio_[0] * 100)\n",
        ")\n",
        "axes[1].set_xlabel(\n",
        "    \"PC1({:.2f}%)\".format(manifold[\"pca\"].explained_variance_ratio_[0] * 100)\n",
        ")\n",
        "axes[2].set_xlabel(\n",
        "    \"PC1({:.2f}%)\".format(manifold[\"pca\"].explained_variance_ratio_[0] * 100)\n",
        ")\n",
        "axes[0].set_ylabel(\n",
        "    \"PC2({:.2f}%)\".format(manifold[\"pca\"].explained_variance_ratio_[1] * 100)\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z0x0H3v-FDl"
      },
      "source": [
        "# t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjd0EDtPMdIW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "manifold = Pipeline([(\"scaler\", StandardScaler()), (\"tsne\", TSNE())])\n",
        "embedding = manifold.fit_transform(X)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "axes[0].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\")\n",
        "axes[1].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y2.values.flatten())\n",
        "axes[2].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y3.values.flatten())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtT6DPD2Kdus"
      },
      "source": [
        "## t-SNE はパラメータによって形が大きく変わってしまう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfoik5RQu8lU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
        "for i, early_exaggeration in enumerate((5, 30, 50)):\n",
        "    for j, perplexity in enumerate((5, 30, 50)):\n",
        "        manifold = Pipeline(\n",
        "            [\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\n",
        "                    \"tsne\",\n",
        "                    TSNE(early_exaggeration=early_exaggeration, perplexity=perplexity),\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        embedding = manifold.fit_transform(X)\n",
        "        axes[i][j].set_title(\n",
        "            \"early_exaggeration={} perplexity={}\".format(early_exaggeration, perplexity)\n",
        "        )\n",
        "        axes[i][j].scatter(\n",
        "            embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\"\n",
        "        )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZvL35y3-gjG"
      },
      "source": [
        "## Isomap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOUGwa92NwvF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import Isomap\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "manifold = Pipeline([(\"scaler\", StandardScaler()), (\"isomap\", Isomap())])\n",
        "embedding = manifold.fit_transform(X)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "axes[0].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\")\n",
        "axes[1].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y2.values.flatten())\n",
        "axes[2].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y3.values.flatten())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnXo5_5HCuaa"
      },
      "source": [
        "## Isomap もまたパラメータによって形が大きく変わってしまう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKv0jfflpIrf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import Isomap\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
        "for i, algorithm in enumerate([\"brute\", \"kd_tree\", \"ball_tree\"]):\n",
        "    for j, n_neighbors in enumerate((5, 30, 50)):\n",
        "        manifold = Pipeline(\n",
        "            [\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\n",
        "                    \"isomap\",\n",
        "                    Isomap(neighbors_algorithm=algorithm, n_neighbors=n_neighbors),\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        embedding = manifold.fit_transform(X)\n",
        "        axes[i][j].set_title(\n",
        "            \"algorithm={} n_neighbors={}\".format(\n",
        "                algorithm,\n",
        "                n_neighbors,\n",
        "            )\n",
        "        )\n",
        "        axes[i][j].scatter(\n",
        "            embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\"\n",
        "        )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWR7Y8omKduu"
      },
      "source": [
        "## UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySp2yAySkxv8"
      },
      "outputs": [],
      "source": [
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMzQ0fNnk4Id"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from umap import UMAP\n",
        "\n",
        "manifold = Pipeline([(\"scaler\", StandardScaler()), (\"umap\", UMAP())])\n",
        "embedding = manifold.fit_transform(X)\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
        "axes[0].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\")\n",
        "axes[1].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y2.values.flatten())\n",
        "axes[2].scatter(embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y3.values.flatten())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7thxyDBKduw"
      },
      "source": [
        "# UMAP もまたパラメータによって形が大きく変わってしまう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aWkWXxllYCK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from umap import UMAP\n",
        "\n",
        "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
        "for i, min_dist in enumerate((0.1, 0.5, 0.9)):\n",
        "    for j, n_neighbors in enumerate((5, 30, 50)):\n",
        "        manifold = Pipeline(\n",
        "            [\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"umap\", UMAP(min_dist=min_dist, n_neighbors=n_neighbors)),\n",
        "            ]\n",
        "        )\n",
        "        embedding = manifold.fit_transform(X)\n",
        "        axes[i][j].set_title(\"min_dist={} n_neighbors={}\".format(min_dist, n_neighbors))\n",
        "        axes[i][j].scatter(\n",
        "            embedding[:, 0], embedding[:, 1], alpha=0.5, c=Y1, cmap=\"jet\"\n",
        "        )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kgWnG-oKdux"
      },
      "source": [
        "# データ分割"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy4dAIX9Kdux"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y1_train, Y1_test = train_test_split(\n",
        "    X, Y1, test_size=0.5, random_state=53\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ7vFxXdKdux"
      },
      "outputs": [],
      "source": [
        "plt.hist(Y1_train, alpha=0.5, label=\"training data\")\n",
        "plt.hist(Y1_test, alpha=0.5, label=\"test data\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5N6Es9QKduy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y2_train, Y2_test = train_test_split(\n",
        "    X, Y2, test_size=0.5, random_state=53\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjuS7tIHKduy"
      },
      "outputs": [],
      "source": [
        "plt.hist(Y2_train, alpha=0.5, label=\"training data\")\n",
        "plt.hist(Y2_test, alpha=0.5, label=\"test data\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6B6_T2eKduy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y3_train, Y3_test = train_test_split(\n",
        "    X, Y3, test_size=0.5, random_state=53, stratify=Y3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHljNLJBKduy"
      },
      "outputs": [],
      "source": [
        "plt.hist(Y3_train, alpha=0.5, label=\"training data\")\n",
        "plt.hist(Y3_test, alpha=0.5, label=\"test data\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7m03Uo-Kduy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(\n",
        "    X_train,\n",
        "    X_test,\n",
        "    Y1_train,\n",
        "    Y1_test,\n",
        "    Y2_train,\n",
        "    Y2_test,\n",
        "    Y3_train,\n",
        "    Y3_test,\n",
        ") = train_test_split(X, Y1, Y2, Y3, test_size=0.5, random_state=53, stratify=Y3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tNIf0reKduz"
      },
      "source": [
        "# 様々な回帰モデルで、様々な前処理の影響を確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOkvJNPPKdu0"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.ensemble import (\n",
        "    ExtraTreesRegressor,\n",
        "    HistGradientBoostingRegressor,\n",
        "    RandomForestRegressor,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    MaxAbsScaler,\n",
        "    MinMaxScaler,\n",
        "    RobustScaler,\n",
        "    StandardScaler,\n",
        ")\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "fig, axes = plt.subplots(nrows=8, ncols=5, figsize=(16, 30))\n",
        "for i, (estimator_name, estimator) in enumerate(\n",
        "    [\n",
        "        [\"PLS\", PLSRegression],\n",
        "        [\"KN\", KNeighborsRegressor],\n",
        "        [\"SVM\", SVR],\n",
        "        [\"DT\", DecisionTreeRegressor],\n",
        "        [\"RF\", RandomForestRegressor],\n",
        "        [\"ET\", ExtraTreesRegressor],\n",
        "        [\"GB\", HistGradientBoostingRegressor],\n",
        "        [\"MLP\", MLPRegressor],\n",
        "    ]\n",
        "):\n",
        "    for j, (scaler_name, scaler) in enumerate(\n",
        "        [\n",
        "            [\"None\", None],\n",
        "            [\"MaxAbs\", MaxAbsScaler],\n",
        "            [\"MinMax\", MinMaxScaler],\n",
        "            [\"Robust\", RobustScaler],\n",
        "            [\"Standard\", StandardScaler],\n",
        "        ]\n",
        "    ):\n",
        "\n",
        "        if scaler is None:\n",
        "            model = estimator()\n",
        "        else:\n",
        "            model = Pipeline([(scaler_name, scaler()), (estimator_name, estimator())])\n",
        "        model.fit(X_train, Y1_train)\n",
        "        score = model.score(X_test, Y1_test)\n",
        "        axes[i][j].set_title(\n",
        "            \"{0} {1} {2:.3f}\".format(scaler_name, estimator_name, score)\n",
        "        )\n",
        "        axes[i][j].scatter(Y1_test, model.predict(X_test), c=Y1_test)\n",
        "        axes[i][j].plot(Y1_test, Y1_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B9tGGQ6Kdu0"
      },
      "source": [
        "# PCAで主成分を抜き出して様々な回帰モデルへの影響を確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3riNgERBKdu1"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import (\n",
        "    ExtraTreesRegressor,\n",
        "    HistGradientBoostingRegressor,\n",
        "    RandomForestRegressor,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    MaxAbsScaler,\n",
        "    MinMaxScaler,\n",
        "    RobustScaler,\n",
        "    StandardScaler,\n",
        ")\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "fig, axes = plt.subplots(nrows=8, ncols=5, figsize=(16, 30))\n",
        "for i, (estimator_name, estimator) in enumerate(\n",
        "    [\n",
        "        [\"PLS\", PLSRegression],\n",
        "        [\"KN\", KNeighborsRegressor],\n",
        "        [\"SVM\", SVR],\n",
        "        [\"DT\", DecisionTreeRegressor],\n",
        "        [\"RF\", RandomForestRegressor],\n",
        "        [\"ET\", ExtraTreesRegressor],\n",
        "        [\"GB\", HistGradientBoostingRegressor],\n",
        "        [\"MLP\", MLPRegressor],\n",
        "    ]\n",
        "):\n",
        "    for j, n_components in enumerate([2, 50, 100, 200, 400]):\n",
        "\n",
        "        model = Pipeline(\n",
        "            [\n",
        "                (\"MinMax\", MinMaxScaler()),\n",
        "                (\"PCA\", PCA(n_components=n_components)),\n",
        "                (estimator_name, estimator()),\n",
        "            ]\n",
        "        )\n",
        "        model.fit(X_train, Y1_train)\n",
        "        score = model.score(X_test, Y1_test)\n",
        "        axes[i][j].set_title(\n",
        "            \"{0} {1} {2:.3f}\".format(n_components, estimator_name, score)\n",
        "        )\n",
        "        axes[i][j].scatter(Y1_test, model.predict(X_test), c=Y1_test)\n",
        "        axes[i][j].plot(Y1_test, Y1_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NSO3FS6Kdu1"
      },
      "source": [
        "# Tree系回帰モデル・分類モデルで特徴量の重要度を確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pQwh8sSKdu7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import (\n",
        "    ExtraTreesClassifier,\n",
        "    ExtraTreesRegressor,\n",
        "    GradientBoostingClassifier,\n",
        "    GradientBoostingRegressor,\n",
        "    RandomForestClassifier,\n",
        "    RandomForestRegressor,\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "\n",
        "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(16, 16))\n",
        "for i, (estimator_name, regressor, classifier) in enumerate(\n",
        "    [\n",
        "        [\"DT\", DecisionTreeRegressor, DecisionTreeClassifier],\n",
        "        [\"RF\", RandomForestRegressor, RandomForestClassifier],\n",
        "        [\"ET\", ExtraTreesRegressor, ExtraTreesClassifier],\n",
        "        [\"GB\", GradientBoostingRegressor, GradientBoostingClassifier],\n",
        "    ]\n",
        "):\n",
        "    for j, (estimator, task, X_tr, X_te, Y_tr, Y_te) in enumerate(\n",
        "        [\n",
        "            [regressor, \"Regr\", X_train, X_test, Y1_train, Y1_test.values.tolist()],\n",
        "            [classifier, \"Clf\", X_train, X_test, Y2_train, Y2_test.values.tolist()],\n",
        "            [classifier, \"Clf\", X_train, X_test, Y3_train, Y3_test.values.tolist()],\n",
        "        ]\n",
        "    ):\n",
        "        model = Pipeline(\n",
        "            [\n",
        "                (\"MinMax\", MinMaxScaler()),\n",
        "                (estimator_name, estimator()),\n",
        "            ]\n",
        "        )\n",
        "        model.fit(X_tr, Y_tr)\n",
        "        score = model.score(X_te, Y_te)\n",
        "        axes[i][j].set_title(\"{0} {1} {2:.3f}\".format(estimator_name, task, score))\n",
        "        topnum = 10\n",
        "        importances = sorted(\n",
        "            [(fi, name) for fi, name in zip(model[1].feature_importances_, X.columns)]\n",
        "        )[::-1]\n",
        "        axes[i][j].barh(\n",
        "            [str(x[1]) for x in importances[:topnum][::-1]],\n",
        "            [x[0] for x in importances[:topnum][::-1]],\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCUwgVj9Kdu8"
      },
      "source": [
        "## 回帰モデルの重要度の算出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkzQXMToKdu9"
      },
      "outputs": [],
      "source": [
        "model = GradientBoostingRegressor()\n",
        "model.fit(X_train, Y1_train)\n",
        "importances = sorted(\n",
        "    [(fi, name) for fi, name in zip(model.feature_importances_, X_train.columns)]\n",
        ")[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dARWm3JVKdu9"
      },
      "source": [
        "# 重要な特徴量を抜き出して様々な回帰モデルへの影響を確認する（特徴選択）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlVvlC8XKdu9"
      },
      "outputs": [],
      "source": [
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import (\n",
        "    ExtraTreesRegressor,\n",
        "    HistGradientBoostingRegressor,\n",
        "    RandomForestRegressor,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    MaxAbsScaler,\n",
        "    MinMaxScaler,\n",
        "    RobustScaler,\n",
        "    StandardScaler,\n",
        ")\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "fig, axes = plt.subplots(nrows=8, ncols=5, figsize=(16, 30))\n",
        "for i, (estimator_name, estimator) in enumerate(\n",
        "    [\n",
        "        [\"PLS\", PLSRegression],\n",
        "        [\"KN\", KNeighborsRegressor],\n",
        "        [\"SVM\", SVR],\n",
        "        [\"DT\", DecisionTreeRegressor],\n",
        "        [\"RF\", RandomForestRegressor],\n",
        "        [\"ET\", ExtraTreesRegressor],\n",
        "        [\"GB\", HistGradientBoostingRegressor],\n",
        "        [\"MLP\", MLPRegressor],\n",
        "    ]\n",
        "):\n",
        "    for j, n_components in enumerate([2, 50, 100, 200, 400]):\n",
        "        selected_columns = [name for fi, name in importances[:n_components]]\n",
        "        model = Pipeline(\n",
        "            [\n",
        "                (\"MinMax\", MinMaxScaler()),\n",
        "                (estimator_name, estimator()),\n",
        "            ]\n",
        "        )\n",
        "        model.fit(X_train[selected_columns], Y1_train)\n",
        "        score = model.score(X_test[selected_columns], Y1_test)\n",
        "        axes[i][j].set_title(\n",
        "            \"{0} {1} {2:.3f}\".format(n_components, estimator_name, score)\n",
        "        )\n",
        "        axes[i][j].scatter(Y1_test, model.predict(X_test[selected_columns]), c=Y1_test)\n",
        "        axes[i][j].plot(Y1_test, Y1_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ohNlLd9Kdu-"
      },
      "source": [
        "# 分類モデルの重要度の算出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8kjnDZ0Kdu-"
      },
      "outputs": [],
      "source": [
        "model = GradientBoostingClassifier()\n",
        "model.fit(X_train, Y3_train)\n",
        "importances = sorted(\n",
        "    [(fi, name) for fi, name in zip(model.feature_importances_, X_train.columns)]\n",
        ")[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgj1HDgxKdu-"
      },
      "source": [
        "# 重要な特徴量を抜き出して様々な分類モデルへの影響を確認する（特徴選択）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RiidFqgKdu_"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import (\n",
        "    ExtraTreesClassifier,\n",
        "    HistGradientBoostingClassifier,\n",
        "    RandomForestClassifier,\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    MaxAbsScaler,\n",
        "    MinMaxScaler,\n",
        "    RobustScaler,\n",
        "    StandardScaler,\n",
        ")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "fig, axes = plt.subplots(nrows=8, ncols=5, figsize=(16, 30))\n",
        "for i, (estimator_name, estimator) in enumerate(\n",
        "    [\n",
        "        [\"LR\", LogisticRegression],\n",
        "        [\"KN\", KNeighborsClassifier],\n",
        "        [\"SVM\", SVC],\n",
        "        [\"DT\", DecisionTreeClassifier],\n",
        "        [\"RF\", RandomForestClassifier],\n",
        "        [\"ET\", ExtraTreesClassifier],\n",
        "        [\"GB\", HistGradientBoostingClassifier],\n",
        "        [\"MLP\", MLPClassifier],\n",
        "    ]\n",
        "):\n",
        "    for j, n_components in enumerate([2, 50, 100, 200, 400]):\n",
        "        selected_columns = [name for fi, name in importances[:n_components]]\n",
        "        model = Pipeline(\n",
        "            [\n",
        "                (\"MinMax\", MinMaxScaler()),\n",
        "                (estimator_name, estimator()),\n",
        "            ]\n",
        "        )\n",
        "        model.fit(X_train[selected_columns], Y3_train)\n",
        "        score = model.score(X_test[selected_columns], Y3_test)\n",
        "        axes[i][j].set_title(\n",
        "            \"{0} {1} {2:.3f}\".format(n_components, estimator_name, score)\n",
        "        )\n",
        "        tn, fp, fn, tp = metrics.confusion_matrix(\n",
        "            Y3_test, model.predict(X_test[selected_columns])\n",
        "        ).ravel()\n",
        "        # axes[i][j].set_title(\"tn, fp, fn, tp = {} {} {} {}\".format(tn, fp, fn, tp))\n",
        "        axes[i][j].bar([\"Positive\", \"Negative\"], [tp, tn])\n",
        "        axes[i][j].bar([\"Positive\", \"Negative\"], [-fn, -fp])\n",
        "        axes[i][j].grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwob5TFU3C5T"
      },
      "source": [
        "# Ensemble （アンサンブル）\n",
        "\n",
        "- voting (複数の学習器を並列に用いて、平等に取り扱う)\n",
        "- stacking (複数の学習器を並列に用いて、その結果をさらに学習する)\n",
        "- bagging (同タイプの学習器を並列に用いて、異なるサンプリングデータで複数回学習する)\n",
        "- boosting (同タイプの学習器を直列に用いて、前の学習器が間違えた部分を後の学習器が再学習する)\n",
        "\n",
        "## Voting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5mGs77d3eKw"
      },
      "outputs": [],
      "source": [
        "from rdkit_supporter import depict\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "model = VotingRegressor(\n",
        "    [(\"svm\", SVR()), (\"kn\", KNeighborsRegressor()), (\"mlp\", MLPRegressor())]\n",
        ")\n",
        "model.fit(X_train, Y1_train)\n",
        "depict.regression_metrics(model, X_test, Y1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xtiPbDZ4UXj"
      },
      "outputs": [],
      "source": [
        "from rdkit_supporter import depict\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = VotingClassifier(\n",
        "    [(\"svm\", SVC()), (\"kn\", KNeighborsClassifier()), (\"mlp\", MLPClassifier())]\n",
        ")\n",
        "model.fit(X_train, Y3_train)\n",
        "depict.classification_metrics(model, X_test, Y3_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lED0g8J1BYWC"
      },
      "source": [
        "## Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SIxhEUmBcEV"
      },
      "outputs": [],
      "source": [
        "from rdkit_supporter import depict\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "model = StackingRegressor(\n",
        "    [(\"svm\", SVR()), (\"kn\", KNeighborsRegressor()), (\"mlp\", MLPRegressor())],\n",
        "    final_estimator=RidgeCV(),\n",
        ")\n",
        "model.fit(X_train, Y1_train)\n",
        "depict.regression_metrics(model, X_test, Y1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs2_HKUMGnbY"
      },
      "outputs": [],
      "source": [
        "from rdkit_supporter import depict\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = StackingClassifier(\n",
        "    [(\"svm\", SVC()), (\"kn\", KNeighborsClassifier()), (\"mlp\", MLPClassifier())],\n",
        "    final_estimator=LogisticRegression(),\n",
        ")\n",
        "model.fit(X_train, Y3_train)\n",
        "depict.classification_metrics(model, X_test, Y3_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wrzGkBl5cGz"
      },
      "source": [
        "## Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T3cDahm5gkA"
      },
      "outputs": [],
      "source": [
        "from rdkit_supporter import depict\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "model = BaggingRegressor(base_estimator=KNeighborsRegressor())\n",
        "model.fit(X_train, Y1_train)\n",
        "depict.regression_metrics(model, X_test, Y1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw1_dhp-6s5V"
      },
      "outputs": [],
      "source": [
        "from rdkit_supporter import depict\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = BaggingClassifier(base_estimator=KNeighborsClassifier())\n",
        "model.fit(X_train, Y3_train)\n",
        "depict.classification_metrics(model, X_test, Y3_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuptGe277ARp"
      },
      "source": [
        "## Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i25i4Aot7Hie"
      },
      "outputs": [],
      "source": [
        "from rdkit_supporter import depict\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "model = AdaBoostRegressor(base_estimator=KNeighborsRegressor())\n",
        "model.fit(X_train, Y1_train)\n",
        "depict.regression_metrics(model, X_test, Y1_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5cOgMqY8iUO"
      },
      "outputs": [],
      "source": [
        "from rdkit_supporter import depict\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "\n",
        "model = AdaBoostClassifier(base_estimator=RandomForestClassifier())\n",
        "model.fit(X_train, Y3_train)\n",
        "depict.classification_metrics(model, X_test, Y3_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FqfHzkw3LyR"
      },
      "source": [
        "# Optuna による多目的最適化ハイパーパラメーターチューニング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tughKLpcKdvD"
      },
      "outputs": [],
      "source": [
        "dateflag = \"0307c\"\n",
        "learning_time_limit = 600\n",
        "timeout_optuna = 60000\n",
        "n_trials_optuna = 1000\n",
        "MODEL_PATH = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO6BXENxjTqm"
      },
      "outputs": [],
      "source": [
        "from functools import wraps\n",
        "\n",
        "\n",
        "# 学習に時間がかかりすぎる場合に強制終了するための方法\n",
        "def on_timeout(limit, handler, hint=None):\n",
        "    def notify_handler(signum, frame):\n",
        "        handler(\n",
        "            \"'%s' terminated since it did not finish in %d seconds.\" % (hint, limit)\n",
        "        )\n",
        "\n",
        "    def __decorator(function):\n",
        "        def __wrapper(*args, **kwargs):\n",
        "            import signal\n",
        "\n",
        "            signal.signal(signal.SIGALRM, notify_handler)\n",
        "            signal.alarm(limit)\n",
        "            result = function(*args, **kwargs)\n",
        "            signal.alarm(0)\n",
        "            return result\n",
        "\n",
        "        return wraps(function)(__wrapper)\n",
        "\n",
        "    return __decorator\n",
        "\n",
        "\n",
        "def handler_func(msg):\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBghfdO7KdvD"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import matthews_corrcoef, precision_score, r2_score, recall_score\n",
        "\n",
        "\n",
        "# Optunaでチューニングするための基本クラス\n",
        "class BestTune:\n",
        "    def __init__(self, x_train, x_valid, t_train, t_valid, task=\"regressor\"):\n",
        "        # 訓練データを格納\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "\n",
        "        # 検証データを格納\n",
        "        self.x_valid = x_valid\n",
        "        self.t_valid = t_valid\n",
        "\n",
        "        # regressor か classifier か\n",
        "        self.task = task\n",
        "        if self.task[0] == \"r\" or self.task[0] == \"R\":\n",
        "            self.measure = r2_score\n",
        "        else:\n",
        "            self.measure = matthews_corrcoef\n",
        "\n",
        "        # ベストモデルとスコアを格納\n",
        "        self.best_score = None\n",
        "        self.best_estimator_ = None\n",
        "\n",
        "    def get_params(self, trial):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_base_model(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @on_timeout(limit=learning_time_limit, handler=handler_func, hint=u\"BestTune\")\n",
        "    def fit(self, trial):\n",
        "        model = self.get_base_model()(**self.get_params(trial))\n",
        "        model.fit(self.x_train, self.t_train)\n",
        "        return model\n",
        "\n",
        "    def __call__(self, trial):\n",
        "        if self.task == \"precision_recall\":\n",
        "            start_time = time.perf_counter()\n",
        "            # 教師データで学習\n",
        "            model = self.fit(trial)\n",
        "\n",
        "            # 検証データの予測性能を評価\n",
        "            precision = precision_score(model.predict(self.x_valid), self.t_valid)\n",
        "            recall = recall_score(model.predict(self.x_valid), self.t_valid)\n",
        "            end_time = time.perf_counter()\n",
        "\n",
        "            # 多目的最適化\n",
        "            return precision, recall, end_time - start_time\n",
        "\n",
        "        else:\n",
        "            start_time = time.perf_counter()\n",
        "            # 教師データで学習\n",
        "            model = self.fit(trial)\n",
        "\n",
        "            # 検証データの予測性能を評価\n",
        "            score = self.measure(model.predict(self.x_valid), self.t_valid)\n",
        "            end_time = time.perf_counter()\n",
        "\n",
        "            # ベストスコアが出れば、そのベストモデルを記録\n",
        "            if self.best_estimator_ is None or self.best_score < score:\n",
        "                self.best_score = score\n",
        "                self.best_estimator_ = copy.deepcopy(model)\n",
        "\n",
        "            # 多目的最適化\n",
        "            return max(-1, score), end_time - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtv3qbu8KdvE"
      },
      "outputs": [],
      "source": [
        "# GradientBoosting\n",
        "\n",
        "from sklearn.ensemble import (\n",
        "    HistGradientBoostingClassifier,\n",
        "    HistGradientBoostingRegressor,\n",
        ")\n",
        "\n",
        "\n",
        "class tune_GB(BestTune):\n",
        "    def get_base_model(self):\n",
        "        if self.task[0] == \"r\" or self.task[0] == \"R\":\n",
        "            return HistGradientBoostingRegressor\n",
        "        else:\n",
        "            return HistGradientBoostingClassifier\n",
        "\n",
        "    def default_params(self):\n",
        "        params = {\n",
        "            \"max_depth\": 100,\n",
        "            \"min_samples_leaf\": 10,\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"max_iter\": 100,\n",
        "            \"max_leaf_nodes\": 31,\n",
        "            \"l2_regularization\": 0.0,\n",
        "            \"max_bins\": 255,\n",
        "            \"early_stopping\": \"auto\",\n",
        "            \"scoring\": \"loss\",\n",
        "        }\n",
        "        if self.task[0] == \"r\" or self.task[0] == \"R\":\n",
        "            params[\"loss\"] = \"squared_error\"\n",
        "        else:\n",
        "            params[\"loss\"] = \"log_loss\"\n",
        "        return params\n",
        "\n",
        "    def get_params(self, trial):\n",
        "        params = {}\n",
        "        params[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
        "        params[\"max_iter\"] = trial.suggest_int(\"max_iter\", 10, 1000)\n",
        "        params[\"max_leaf_nodes\"] = trial.suggest_int(\"max_leaf_nodes\", 10, 100)\n",
        "        params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 200)\n",
        "        params[\"min_samples_leaf\"] = trial.suggest_int(\"min_samples_leaf\", 1, 100)\n",
        "        params[\"l2_regularization\"] = trial.suggest_float(\"l2_regularization\", 0, 10)\n",
        "        params[\"max_bins\"] = trial.suggest_int(\"max_bins\", 2, 255)\n",
        "        params[\"interaction_cst\"] = trial.suggest_categorical(\n",
        "            \"interaction_cst\", [\"pairwise\", \"no_interactions\"]\n",
        "        )\n",
        "        if self.task[0] == \"r\" or self.task[0] == \"R\":\n",
        "            params[\"loss\"] = trial.suggest_categorical(\n",
        "                \"loss\", [\"squared_error\", \"absolute_error\"]\n",
        "            )\n",
        "        else:\n",
        "            params[\"loss\"] = trial.suggest_categorical(\n",
        "                \"loss\", [\"auto\", \"binary_crossentropy\", \"log_loss\"]\n",
        "            )\n",
        "        return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BYnybzOKdvE"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "\n",
        "# Optuna で学習を繰り返し、学習履歴を保存する\n",
        "def train(\n",
        "    study_name,\n",
        "    tune_model,\n",
        "    timeout=timeout_optuna,\n",
        "    n_trials=n_trials_optuna,\n",
        "    show_progress_bar=True,\n",
        "):\n",
        "    import warnings\n",
        "\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARN)\n",
        "\n",
        "    # 学習環境を立ち上げる\n",
        "    study = optuna.create_study(\n",
        "        study_name=study_name,\n",
        "        storage=\"sqlite:///\" + study_name + \".sql\",\n",
        "        load_if_exists=True,\n",
        "        directions=[\"maximize\", \"maximize\", \"minimize\"],\n",
        "        sampler=optuna.samplers.NSGAIISampler(),\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        study.enqueue_trial(study.best_trial.params)\n",
        "    except:\n",
        "        try:\n",
        "            study.enqueue_trial(tune_model.default_params())\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # 学習する\n",
        "    study.optimize(\n",
        "        tune_model,\n",
        "        timeout=timeout,\n",
        "        n_trials=n_trials,\n",
        "        show_progress_bar=show_progress_bar,\n",
        "    )\n",
        "    return study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLmB_QbHKdvF"
      },
      "outputs": [],
      "source": [
        "strage_name = \"GBR_{}\".format(dateflag)\n",
        "study = train(\n",
        "    \"{}{}_bestfit\".format(MODEL_PATH, strage_name),\n",
        "    tune_GB(X_train, X_test, Y3_train, Y3_test, task=\"precision_recall\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKZXmAffKdvF"
      },
      "source": [
        "## 学習履歴の表示（すべてのトライアル）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMQ3cXijKdvF"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for trial in study.trials:\n",
        "    if trial.values is not None:\n",
        "        params = trial.params\n",
        "        params[\"trial_number\"] = trial.number\n",
        "        params[\"precision\"] = trial.values[0]\n",
        "        params[\"recall\"] = trial.values[1]\n",
        "        params[\"time\"] = trial.values[2]\n",
        "        results.append(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j3ZvJwHKdvG"
      },
      "outputs": [],
      "source": [
        "for xlabel in sorted(params.keys()):\n",
        "    if xlabel == \"trial_number\":\n",
        "        continue\n",
        "    for ylabel in [\"precision\", \"recall\", \"time\"]:\n",
        "        if ylabel == \"trial_number\":\n",
        "            continue\n",
        "        if xlabel == ylabel:\n",
        "            continue\n",
        "        plt.scatter(\n",
        "            [record[xlabel] for record in results],\n",
        "            [record[ylabel] for record in results],\n",
        "            c=[record[\"trial_number\"] for record in results],\n",
        "            cmap=\"Blues\",\n",
        "        )\n",
        "        plt.grid()\n",
        "        plt.colorbar()\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LAdnr02KdvG"
      },
      "source": [
        "## 学習履歴の表示（ベストトライアル）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMveE59RKdvG"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for trial in study.best_trials:\n",
        "    if trial.values is not None:\n",
        "        params = trial.params\n",
        "        params[\"trial_number\"] = trial.number\n",
        "        params[\"precision\"] = trial.values[0]\n",
        "        params[\"recall\"] = trial.values[1]\n",
        "        params[\"time\"] = trial.values[2]\n",
        "        results.append(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bVRVeXSKdvH"
      },
      "outputs": [],
      "source": [
        "for xlabel in sorted(params.keys()):\n",
        "    if xlabel == \"trial_number\":\n",
        "        continue\n",
        "    for ylabel in [\"precision\", \"recall\", \"time\"]:\n",
        "        if ylabel == \"trial_number\":\n",
        "            continue\n",
        "        if xlabel == ylabel:\n",
        "            continue\n",
        "        plt.scatter(\n",
        "            [record[xlabel] for record in results],\n",
        "            [record[ylabel] for record in results],\n",
        "            c=[record[\"trial_number\"] for record in results],\n",
        "            cmap=\"Blues\",\n",
        "        )\n",
        "        plt.grid()\n",
        "        plt.colorbar()\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB_V8zClKdvH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}